{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8244ef43-a0a4-4ca0-a206-e0a33e3436ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+------------+\n| id|   name|salary|country|copiedsalary|\n+---+-------+------+-------+------------+\n|  1|    Jai| 40000|  India|       40000|\n|  2| Raghav| 60000|  India|       60000|\n|  3|Prakhar| 80000|  India|       80000|\n+---+-------+------+-------+------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- country: string (nullable = false)\n |-- copiedsalary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "data = [(1,'Jai','20000'),\n",
    "        (2,'Raghav','30000'),\n",
    "        (3,'Prakhar','40000')]\n",
    "\n",
    "column = ['id','name','salary']\n",
    "df = spark.createDataFrame(data=data,schema=column)\n",
    "\n",
    "df1 = df.withColumn(colName='salary',col=col('salary').cast('Integer')) #when we want to change the datatype of any column.\n",
    "df2 = df1.withColumn('salary',col('salary')*2) # update existing value with column.\n",
    "df3 = df2.withColumn('country',lit('India')) # we create new column with existing value.\n",
    "df4 = df3.withColumn('copiedsalary',col('salary'))\n",
    "\n",
    "df4.show()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758f6416-0ed8-4138-997a-852c29fad72c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n| id|   name|salary_amount|\n+---+-------+-------------+\n|  1|    Jai|        20000|\n|  2| Raghav|        30000|\n|  3|Prakhar|        40000|\n+---+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#How to renamed column name \n",
    "\n",
    "data = [(1,'Jai','20000'),\n",
    "        (2,'Raghav','30000'),\n",
    "        (3,'Prakhar','40000')]\n",
    "\n",
    "column = ['id','name','salary']\n",
    "df = spark.createDataFrame(data=data,schema=column)\n",
    "df1 = df.withColumnRenamed('salary','salary_amount')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "943c2b4a-cd50-47b3-a3c8-8d8e587675c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "withColumnNotebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
