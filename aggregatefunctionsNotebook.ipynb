{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8c18fb-a866-41ac-b54f-928c73fdae1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n|   name|dep|salary|\n+-------+---+------+\n|   Ashu| IT| 20000|\n|   Ashi| HR| 30000|\n|    Jai|WPS| 40000|\n|Prakhar| IT| 20000|\n+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "sampleData = [('Ashu','IT',20000),\n",
    "              ('Ashi','HR',30000),\n",
    "              ('Jai','WPS',40000),\n",
    "              ('Prakhar','IT',20000)]\n",
    "\n",
    "schema = ['name','dep','salary']\n",
    "df = spark.createDataFrame(data=sampleData,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50b8ed1-da93-42ee-b4e6-6c00c4ab8aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|approx_count_distinct(salary)|\n+-----------------------------+\n|                            3|\n+-----------------------------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|    27500.0|\n+-----------+\n\n+----------------------------+\n|collect_list(salary)        |\n+----------------------------+\n|[20000, 30000, 40000, 20000]|\n+----------------------------+\n\n+---------------------+\n|collect_set(salary)  |\n+---------------------+\n|[40000, 30000, 20000]|\n+---------------------+\n\n+----------------------+\n|count(DISTINCT salary)|\n+----------------------+\n|                     3|\n+----------------------+\n\n+-------------+\n|count(salary)|\n+-------------+\n|            4|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg, collect_list, collect_set, countDistinct, count\n",
    "\n",
    "df.select(approx_count_distinct('salary')).show()\n",
    "df.select(avg('salary')).show()\n",
    "df.select(collect_list('salary')).show(truncate=False)\n",
    "df.select(collect_set('salary')).show(truncate=False)\n",
    "df.select(countDistinct('salary')).show()\n",
    "df.select(count('salary')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b022dca-59ea-4ce1-b87d-e6c260b61466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+\n|    name|    dep|salary|\n+--------+-------+------+\n|    Annu|     HR| 50000|\n|    Ashi|     HR| 30000|\n|  Anjali|     HR| 60000|\n|    Ashu|     IT| 20000|\n|Abhishek|     IT| 50000|\n| Prakhar|     IT| 20000|\n|  Saumya|     IT| 40000|\n|     Adi|Payroll| 60000|\n|  Akarsh|Payroll| 70000|\n|     Jai|    WPS| 40000|\n+--------+-------+------+\n\n+--------+-------+------+---------+----+----------+\n|    name|    dep|salary|rowNumber|rank|dense_rank|\n+--------+-------+------+---------+----+----------+\n|    Ashi|     HR| 30000|        1|   1|         1|\n|    Annu|     HR| 50000|        2|   2|         2|\n|  Anjali|     HR| 60000|        3|   3|         3|\n|    Ashu|     IT| 20000|        1|   1|         1|\n| Prakhar|     IT| 20000|        2|   1|         1|\n|  Saumya|     IT| 40000|        3|   3|         2|\n|Abhishek|     IT| 50000|        4|   4|         3|\n|     Adi|Payroll| 60000|        1|   1|         1|\n|  Akarsh|Payroll| 70000|        2|   2|         2|\n|     Jai|    WPS| 40000|        1|   1|         1|\n+--------+-------+------+---------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sampleData1 = [('Ashu','IT',20000),\n",
    "              ('Ashi','HR',30000),\n",
    "              ('Jai','WPS',40000),\n",
    "              ('Prakhar','IT',20000),\n",
    "              ('Annu','HR',50000),\n",
    "              ('Abhishek','IT',50000),\n",
    "              ('Adi','Payroll',60000),\n",
    "              ('Akarsh','Payroll',70000),\n",
    "              ('Anjali','HR',60000),\n",
    "              ('Saumya','IT',40000)]\n",
    "\n",
    "schema = ['name','dep','salary']\n",
    "df = spark.createDataFrame(data=sampleData1,schema=schema)\n",
    "#df.show()\n",
    "df.sort('dep').show()\n",
    "\n",
    "window = Window.partitionBy('dep').orderBy('salary')\n",
    "\n",
    "#Rank skip identical rank but dense_rank not skip identical rank\n",
    "df.withColumn('rowNumber', row_number().over(window)).\\\n",
    "    withColumn('rank', rank().over(window)).\\\n",
    "    withColumn('dense_rank', dense_rank().over(window)).show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ec4346-0612-457a-992d-ac803d0d0b55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aggregatefunctionsNotebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
