{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4691f4d3-9a6a-43de-9b60-9200cc514029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Ashu</td></tr><tr><td>2</td><td>Jai</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ashu"
        ],
        [
         2,
         "Jai"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1,'Ashu'),(2,'Jai')]\n",
    "schema = ['id','name']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751cf9c8-4305-4f66-bd9b-cbf051fa9cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameWriter in module pyspark.sql.readwriter object:\n\nclass DataFrameWriter(OptionUtils)\n |  DataFrameWriter(df: 'DataFrame')\n |  \n |  Interface used to write a :class:`DataFrame` to external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n |  to access this.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Method resolution order:\n |      DataFrameWriter\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: 'DataFrame')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Buckets the output by the given columns. If specified,\n |      the output is laid out on the file system similar to Hive's bucketing scheme,\n |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      numBuckets : int\n |          the number of buckets to save\n |      col : str, list or tuple\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Notes\n |      -----\n |      Applicable for file-based data sources in combination with\n |      :py:meth:`DataFrameWriter.saveAsTable`.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n |  \n |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n |              exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a CSV file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.csv(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  format(self, source: str) -> 'DataFrameWriter'\n |      Specifies the underlying output data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.format('parquet')\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format('parquet').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n |      Inserts the content of the :class:`DataFrame` to the specified table.\n |      \n |      It requires that the schema of the :class:`DataFrame` is the same as the\n |      schema of the table.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      overwrite : bool, optional\n |          If true, overwrites existing data. Disabled by default\n |      \n |      Notes\n |      -----\n |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n |      the column names and just uses position-based resolution.\n |      \n |      Examples\n |      --------\n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> df = spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... )\n |      >>> df.write.saveAsTable(\"tblA\")\n |      \n |      Insert the data into 'tblA' table but with different column names.\n |      \n |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Name of the table in the external database.\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      properties : dict\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |  \n |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n |      Saves the content of the :class:`DataFrame` in JSON format\n |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n |      specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.json(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format(\"json\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n |      Specifies the behavior when data or table already exists.\n |      \n |      Options include:\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Examples\n |      --------\n |      Raise an error when writing to an existing path.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n |      Traceback (most recent call last):\n |          ...\n |      ...AnalysisException: ...\n |      \n |      Write a Parquet file back with various options, and read it back.\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Overwrite the path with a new Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |120|Takuya Ueshin|\n |      |100| Hyukjin Kwon|\n |      +---+-------------+\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds an output option for the underlying data source.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key for the option to set.\n |      value\n |          The value for the option to set.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' with writing a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds output options for the underlying data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      **options : dict\n |          The dictionary of string keys and primitive-type values.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n |      \n |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n |      >>> schema = StructType([\n |      ...     StructField(\"age\",IntegerType(),True),\n |      ...     StructField(\"name\",StringType(),True),\n |      ... ])\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n |      ...         \"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a ORC file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a ORC file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.orc(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"orc\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.parquet(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"parquet\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n |      Partitions the output by the given columns on the file system.\n |      \n |      If specified, the output is laid out on the file system similar\n |      to Hive's partitioning scheme.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      cols : str or list\n |          name of columns\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n |      \n |      >>> import tempfile\n |      >>> import os\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).sort(\"age\").show()\n |      ...\n |      ...     # Read one partition as a DataFrame.\n |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |100| Hyukjin Kwon|\n |      |120|Ruifeng Zheng|\n |      +---+-------------+\n |      +---+\n |      |age|\n |      +---+\n |      |100|\n |      +---+\n |  \n |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the contents of the :class:`DataFrame` to a data source.\n |      \n |      The data source is specified by the ``format`` and a set of ``options``.\n |      If ``format`` is not specified, the default data source configured by\n |      ``spark.sql.sources.default`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str, optional\n |          the path in a Hadoop supported file system\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : list, optional\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format('json').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the content of the :class:`DataFrame` as the specified table.\n |      \n |      In the case the table already exists, behavior of this function depends on the\n |      save mode, specified by the `mode` function (default to throwing an exception).\n |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n |      the same as that of the existing table.\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Notes\n |      -----\n |      When `mode` is `Append`, if there is an existing table, we will use the format and\n |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n |      doesn't need to be the same as that of the existing table. Unlike\n |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n |      column names to find the correct column positions.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          the table name\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n |      partitionBy : str or list\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Creates a table from a DataFrame, and read it back.\n |      \n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.saveAsTable(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Sorts the output in each bucket by the given columns on the file system.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      col : str, tuple or list\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n |  \n |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the DataFrame in a text file at the specified path.\n |      The text files will be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      The DataFrame must have only one column that is of string type.\n |      Each row becomes a new line in the output file.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a text file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a text file\n |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n |      ...     df.write.mode(\"overwrite\").text(d)\n |      ...\n |      ...     # Read the text file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n |      +---------+\n |      |alphabets|\n |      +---------+\n |      |        a|\n |      |        b|\n |      |        c|\n |      +---------+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(df.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebef8b20-53cb-4eac-9780-62e4ad217de2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.json('dbfs:/FileStore/jsondata/emps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a83ed70-64c6-46ee-b089-2991728f4230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Ashu</td></tr><tr><td>2</td><td>Jai</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ashu"
        ],
        [
         2,
         "Jai"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.json('dbfs:/FileStore/jsondata/emps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e402930b-6353-4f74-a67a-e1a8a54f97ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "writeDFintoJson",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
